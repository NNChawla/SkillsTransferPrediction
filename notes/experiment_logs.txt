Have experimented across the following parameters:
- Segment size (full sequence or 1 second intervals)
- Enabled trackers (Head, Left Hand, Right Hand)
- Enabled measurements (combinations of position, euler, quat, sixD)
- Enabled metadata (combinations of build time and gender)
- Handle NaN (impute or drop)
- Train set (A or B)
- Sample rate (None or 20 seconds)

Predicting:
- Regression (raw score): RMSE, MAE, R2, MSE
- Classification (category): Accuracy, F1-Score, Precision, Recall

Version 1 (v1)
Parallelized both regression and classification experiments. ~5-6 minutes per ~800-900 experiments.
Testing with full segment size only, imputing NaN values only, and no sample rate (using full sequence).

Findings:
In both classification and regression, the left hand seems to be the key tracker. I think some combination of build time and the rotation representations could be helpful as well; can't tell yet.
Better results testing on A when training on B, not by a lot but makes sense given users completed A before B.
Scores for both are very low, there are a lot of potential reasons for this.
- Only using kNNs, could explore GBMs, Random Forests, SVMs for classification.
- Need to verify that the data is being normalized and scaled without issues.
- Not tuning the hyperparameters at all, could be helpful.
- Feature engineering could be useful, lot of potential features to be examined.

Thoughts:
- Consider the relationship between the features; how linear are they, does it make sense that one model would be better than another?
- Can probably automate the feature combination process, rather than manually specifying them.
- Using a config file for the experiment parameters would be helpful.
    - Specifying models, metrics, etc.
    - Feature combinations should be generated based on the ones specified in the config file.
- Generating some visuals for the results would be helpful.
- Can probably use a single approach for both regression and classification, rather than two separate ones.
    - Just need to use classes to separate the two, and then use a single pipeline for the experiments.
    - Need to make the pipeline flexible, more modular, including the ability to add new models, metrics, etc.

Docket:
1. Separate feature specification to config file
2. Automatically generate all feature combinations in config file, keeping x, y, and z components together for features like position and euler.
3. Unify classification and regression experiments into a single pipeline, using classes to separate the two.
4. Make the pipeline flexible, more modular, including the ability to add new models, metrics, etc.
5. Update the architecture so that derived/engineered features can easily be added to the pipeline and specified in the config file.
6. Engineer kinematic and motion features
7. Add new derived features to config file rerun experiments for v2

Version 2 (v2)
Testing with train policy A only, segment size full, sample rate null, and no metadata.
Beginning with position, quat, velocity, and angular velocity.
My logic is that the metadata has not had a significant effect so far, and it seems as if (and makes sense that) good performance training on A will indicate good performance on B.
Was running out of RAM due to large size of each csv as number of features increases (30mb per file). Need to optimize loading, considering converting CSVs to Parquets.

Key Finding: Memory isn't the issue; number of combinations is just exponentially crazy. Improved speed; roughly 3 experiments per second now.
Key Finding: future direction is to better understand the feature space. Postpone engineering until pipeline to better understand existing features is established.

Backup to github
Need to load the data in and then export the combined dataframe to a single file for analysis.
Need a separate script where feature distribution and correlation (VIF and correlation matrix) can be investigated.
Full EDA (check chatgpt logs) pipeline, looking at data from start to finish.
Train a random forest on all variables and explore the SHAP values.

LightGBM
Scene-Relative Thoughts
Best performance so far (-0.506) with Left Hand angular velocity only.
Tested other individual variables, but performance was worse.
Improvement in performance is significant when dropping the head vs hand trackers.
General thoughts are:
    - Position and rotation may be largely redundant for this model at least.
    - Head Tracker seems to be contributing noise, but not sure what particular signal.
    - Angular Velocity seems more informative than linear velocity.

Body-Relative Thoughts
So big surprise, the pause duration features and gender yield the best performance so far?
Using a feature by itself yields the best performance so far I think.
Unsure of how much the use of all summary statistics is contributing to this.
Body relative transformation does not seem to yield better performance, barely worse but not worth the effort then.
Maybe not strictly the case though; Left hand body relative time since pause yields 0.365, which ~5% better than scene relative.
Right hand pause duration yields the best performance so far (-0.317).
Head signals seem largely noisey.
Pairing gender with right hand pause duration actually yields better performance (-0.309).
Why would body relative pause duration/time since pause not do as well when paired with gender?
Still need to look at X, Y, and Z components of all the motion features individually. Waiting till v3 to adjust summary statistics.

Need to test with kNN and random forest, but going to wait till v3.

V3 docket:
    - Need to update config and scripts to remove tracker and measurement combinations, switch all to global features.
    - All features (metadata, global) should be able to specify which summary statistics to use.
    - Acceleration and jerk features should be added, to see if even further improvement can be had, since velocity was better than position.
    - Need to be able to specify what features to remain constant in combinations
    - Need to be able to specify what combination sizes to use
    - Need to separate out motion feature x,y,z components; each should be an individual feature so they can be studied and summary stats can be varied appropriately.

Version 3 (v3)

Conducting a test with all features, but only using one combination size.
Going for a greedy approach, starting with the most important features and then adding more as needed.
Tested MLP, SVM, Random Forest, LightGBM, and KNN on combination size 1.
Got first -0.2* and -0.1* R^2 scores!
-0.2* scores are from MLP, lgbm, and knn.
-0.1* score is from knn (right hand velocity x mean).
Seems like the top features are right hand velocity x mean, gender, pause duration. Angular and Linear velocity are meaningful.
Need to add acceleration and jerk features then test again.
Disabling relative features for now, will explore again if necessary.
Minimal initial gains from acceleration and jerk. Think the move forward is now EDA and feature selection.

Key Finding: Need to explore transformations (log, sqrt, square, etc.) to observe non-linear effects.
Key Finding: Need to investigate linear correlation and linear dependence (multicollinearity) of features.

Use VIF for multicollinearity, pearson correlation for linear correlation. Mutual information for non-linear correlation.
Ran a 2 combination size experiment with right hand velocity x mean as the required global feature.
Baseline performance by itself with lightgbm was -0.2488. 6 combinations broke -0.2 barrier.
These were:
    - RightHand_quat_x_mean
    - RightHand_quat_z_median (-0.179, best performer)
    - RightHand_quat_z_mean
    - RightHand_velocity_z_min
    - RightHand_jerk_z_min
    - Head_jerk_pause_duration_max

Exported new feature table (summary stats for all features on all participants) to csv at results/feature_exports/features_A.csv
Adding utils for scatter plot generation, correlation matrix, pearson correlation, VIF, and mutual information using the feature table.
Will analyze for linear/nonlinear correlation, multicollinearity, etc. as previously stated.
Initial analysis shows that there are some slight but apparent trends in features that are performing well.
However, the relationship is clearly not linear, so there multiple features will be necessary to account for the variance in the score.

Key Finding: Outliers need to be controlled for.
Key Finding: Noise in motion features might benefit from some form of filtering like smoothing.

Note: General rule is around sqrt(N) to N-1 features for small datasets. So 10-100 features is potentially acceptable.

Docket:
    - Time spent above threshold/below threshold for multiple features (velocity, etc)
    - Time spent in quadrant for position and euler
    - Examine and apply transformations
    - Aggregate features
    - Remove features with poor variance, correlation, and dependence

v4 (Version 4) [Hurdle Model]:
Applied boxcox transformations across the board; minimal gains and losses.
Clear that through simple transformations, combinations, and statistics, little performance can be gained.
Instead requires reframing of the problem, now taking a hurdle model approach.
The score has a high number of true 0s, so a hurdle model dichotomizes the score into 0 and non-0.
We train a binary classifier to predict whether the score is 0 or not, and then train a regression model to predict the score given that it is not 0.

So far, binary classification has yielded 0.904 accuracy for most velocity and quaternion features.
However, precision is around 0.8 for most features, so there is room for improvement.
RightHand_quat_y_mean and Head_quat_y_mean have slightly lower accuracy and recall (0.895), but higher precision and f1 score.
Around 1% higher than the rest in f1 score (0.8688 to 0.8595) but almost 4% higher in precision (0.857 to 0.819).
Low precision says that a participant will have a non-zero error rate when they will not, which of low consequence.
Of higher consequence is low recall, which says that a participant will have a zero error rate when they should not.
Will seek to improve the classifier before proceeding with the regression model.
This has been done with a lightgbm classifier, so other models still need to be explored as well.

"""The negative class (label 'Non-0') represents scores greater than 0
The positive class (label '0') represents scores of exactly 0
This means when interpreting metrics like precision, recall, and F1-score:
True Positives are correctly identified zero scores
True Negatives are correctly identified non-zero scores
False Positives are non-zero scores incorrectly classified as zero
False Negatives are zero scores incorrectly classified as non-zero"""

kNN: RightHand_quat_w_mean and Head_velocity_z_mean have 0.914 accuracy, 0.881 f1 score, 0.92 precision, and 0.914 recall.
tabPFN and SVM: uniform results, nothing outstanding. Maybe better with more features then?

Boxcox on hurdle model regressor score could reduce skew and improve performance.
Right hand velocity x median and A build time get 92% accuracy and 0.9086 f1 score.
More quat or velocity features though reduce these scores.
Really high mutual information from angular velocity and euler features, which makes sense but hasn't shown in the models.

Supervised PCA didn't provide any out-of-box improvements.
Neither did unsupervised PCA.


Body relative -> head is shifting origin (parent of transform)
Dominant hand relative -> Right hand is shifting origin (parent of transform)
Segment approach is not out of box best, but it does provide a way to get more data.
Need to review mutual information with the segment approach and 0 vs non-0.
Logistic regression with elastic net regularization is a good candidate for the hurdle model.
Maybe explore Hi/Lo rather than 0 vs non-0 (more data for each class)
Cross-validation should be checked out for within A, within segment
Update output of score from multiple of 2 to 1
Email ryan to ask for video and step data
Sliding window approach rather than fixed segment
Separate by object (head, controller)
Experiment with thresholds and modes for segmenting
Maybe prevent undersampling majority since already low number of samples
Experiment with other segment level features
Could position of segment in series be useful? (Start, middle , end)
Maybe number of segments is a feature
Consider pauses as opposed to velocity segments, and pause duration as a feature
Component (axis) level thresholding
Pos rotation etc for segment level features
Time series prediction as feature? predicting next segment or confidence of next segment as a feature (kind of like a PRM)

--------------------------------

Nested cross validation to prevent overfitting on noise
Use percent classified correctly for each class as performance heuristic rather than strictly roc auc or pr

Use LOOCV cross validation due to small dataset size
Embedded (linear model + regularization)
or Sequential with SVM or GBM
Balanced accuracy or f1 score as scoring objective due to class imbalance
Hyperparameter tuning for each fold
When all combined should be enough to prevent overfitting alongside domain knowledge of features

IMPORTANT: Use confidence of classifier prediction (predict_proba) to discern model quality. Also to use as input feature to regressor (probability the sample is perfect or not)
IMPORTANT: Ensemble left hand, right hand, head specific classifiers/regressors? (Only given features for each object, majority voting of all 3 models used).

Most common advice is to use domain knowledge. With small dataset bound to overfit if too many features, so hand pick those that make sense

Verify representations of body and head
Comparison of representations

Notes on full battery of experiments for knn:
    - Top features (>0.24 mcc, top is 0.25 mcc)
    - Parameters (model[knn], dataset[v3, BodyRelative_Motion_PQ], cross_task[True, False],
     outer_cv_k[5, 10], inner_cv_k[4, 5], use_resampling[True, False], cls_score_threshold[0, 4, 6, 8], train_policy[A, B])
    - datasets: 50% v3, 50% BodyRelative_Motion_PQ | 0.25 mcc v3
    - cross_task: 50% True, 50% False | 0.25 mcc False
    - outer_cv_k: 6/8 5, 2/8 10 | 0.25 mcc 5
    - inner_cv_k: 6/8 5, 2/8 4 | 0.25 mcc 4 | top 2 were 4
    - use_resampling: 50% True, 50% False | 0.25 mcc True
    - cls_score_threshold: 50% 0, 50% 6 | 0.25 mcc 6
    - train_policy: 50% A, 50% B | 0.25 mcc B

Linearly shifting break point to find optimal split between scores [Done]
Matthews Correlation Coefficient [Done]
Add cross validation to single run for cross task [Done]
Add nested cross validation to single run for within task [Done]
Add hyperparameter optimization for single run [Done]
Change lists to variables [Done]
Add within vs cross task for single run [Done]
Test both ways (A->B and B->A) for single_run [Done]
^ run_experiments add looping through datasets, training sets, other parameters [Done]
make required features and combination sizes only when manual [Done]
Separate classification predictor from regression predictor and score_prediction_base [Done]
add functional results logging using table [Done]
recalculate scores and output as single point rather than actual cost for each operation [Done]
return results from run_experiments and compile into table in run_experiments.py [Done]

Correlation matrix of features for all representations {scene complete (v3), head dominant, hand dominant}
Analyze results of knn and random forest for cross task and within task
Test knn and random forest on linear score
Fix SVM classification
One hot encode categorical features
Optuna (on best model) after decent performance is achieved; hyperparameters are just to optimize performance
Add suggested hyperparameters as well as optional hyperparameters (class balance)
Do A->B and B->A for manual
    - Manual testing may work better because I highly deprioritize variables from the same object-axis pairing (i.e. Head_quat_x_mean, Head_quat_x_median, etc.)
    - This prevents overfitting on noise
    - Manual testing needs to be done over all important parameters in run_experiments.py, just like automatic
    - Cannot sustainably pore over all confusion matrices; need to use multi-level scoring
        - Begin with Sensitivity (TP / TP + FN), once satisfactory level achieved, maintain while maximizing Specificity (TN / TN + FP)
    - Need to maintain cross validation (repeated? stratified k fold)

Ensemble 9 models, one trained on each substep (all actions for each substep) for each participant.
Refactor to separate initialization, training, andvalidation phases into separate methods.

vvv Features and Representations vvv
Unified hand representation
    - Right hand dominant feature predictions will always exclude the left hand learners, so a unified represenation might help (ask GPT)
Segment level features:
    - Pause duration
    - Start, middle, end specific summaries of all statistics
    - Number of segments given threshold
    - Time spent below/above threshold (position, speed, etc)
    - Quadrant time analysis (position)
Substep features:
    - Frequency of step
    - Time between steps
    - start middle end substep rate
Variance of speed between periods (start, middle, end) or substeps segments?
Ensemble left, right, head specific models

Temporal features:
    - Total completion time: How long did it take each user to finish the tutorial?
    - Average step time: The mean duration per step.
    - Variance/standard deviation of step times: Do some users have more consistent timing? Consistency might indicate a clear understanding of the task.
    - Time per critical step: If some steps are known to be more “memory–demanding” later on (e.g., those that build up the structure), then the time taken on these may be especially informative.
    - Time between high activity periods
    - Pause time between substeps (could indicate taking more time to absorb steps)
    - Pause time at end or beginning of tutorial or in each segment (start, middle, end)?
    - Time differences between successive steps: These “delta times” might show hesitation or a need for correction. For example, a sudden increase in time might indicate uncertainty.
    - Learning curve indicators: If you can compute the change in step times from the beginning to the end of the tutorial, that might reflect how quickly a user “gets it” – perhaps a steep improvement correlates with a more robust memory of the procedure.

Edit distance / sequence alignment: Compare the sequence of actions (e.g., “place pipe,” “rotate connector,” “remove pipe,” etc.) with the ideal tutorial sequence. A larger edit distance (or more “corrections”) may signal that the user did not internalize the procedure perfectly.
Count of corrections or “undo” actions: A high count might indicate that the user was uncertain, which may translate to poorer memory of the structure.
Deviations from the ideal sequence: Not just the count of errors, but when they occur (early vs. late in the sequence) may be predictive.
Path efficiency: If users take a convoluted route to complete a step (e.g., unnecessary movements), that might indicate cognitive load.

Transition between levels of fidelity: 2d game, 2d without color

UMAP embeddings for visualization and as features
Wavelet transform for time series features
Dynamic time warping for gold standard time series comparison
Poincare sections for motion features
Markov chains for step transition

Grok feature analysis and engineering
Features in Notes